<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="referrer" content="origin" />
    <meta property="og:description" content="Spark SQL原理解析前言： Spark SQL源码剖析（一）SQL解析框架Catalyst流程概述 Spark SQL源码解析（二）Antlr4解析Sql并生成树 Spark SQL源码解析（三" />
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Spark SQL源码解析（五）SparkPlan准备和执行阶段 - zzzzMing - 博客园</title>
    <link rel="shortcut icon" href="//common.cnblogs.com/favicon.ico?v=20200522" type="image/x-icon" />
    
    <link rel="stylesheet" href="/css/blog-common.min.css?v=KCO3_f2W_TC__-jZ7phSnmoFkQuWMJH2yAgA16eE3eY" />
    <link id="MainCss" rel="stylesheet" href="/skins/simplememory/bundle-simplememory.min.css?v=OL4qeo1LNGlN1rKIhv5UctANvt0M6Nx6kLzr_ffx3Xk" />
    
    <link id="mobile-style" media="only screen and (max-width: 767px)" type="text/css" rel="stylesheet" href="/skins/SimpleMemory/bundle-SimpleMemory-mobile.min.css" />
    
    <link type="application/rss+xml" rel="alternate" href="https://www.cnblogs.com/listenfwind/rss" />
    <link type="application/rsd+xml" rel="EditURI" href="https://www.cnblogs.com/listenfwind/rsd.xml" />
    <link type="application/wlwmanifest+xml" rel="wlwmanifest" href="https://www.cnblogs.com/listenfwind/wlwmanifest.xml" />
    <script src="https://common.cnblogs.com/scripts/jquery-2.2.0.min.js"></script>
    <script src="/js/blog-common.min.js?v=6bwfCY2e02dLOXNW99G2BHZkYFmw9QyYTWeJ-W-sudo"></script>
    <script>
        var currentBlogId = 303786;
        var currentBlogApp = 'listenfwind';
        var cb_enable_mathjax = false;
        var isLogined = false;
        var skinName = 'SimpleMemory';
    </script>
    
    
    
</head>
<body>
    <a name="top"></a>
    <div id="page_begin_html">
        <div style="display: none"><p>
<span>java/Java</span>
<span>Pyton</span>
<span>大数据</span>
<span>Hadoop</span>
<span>Spark</span>
</p></div>
    </div>
    
<!--done-->
<div id="home">
<div id="header">
	<div id="blogTitle">
        <a id="lnkBlogLogo" href="https://www.cnblogs.com/listenfwind/"><img id="blogLogo" src="/skins/custom/images/logo.gif" alt="返回主页" /></a>		
		
<!--done-->
<h1><a id="Header1_HeaderTitle" class="headermaintitle HeaderMainTitle" href="https://www.cnblogs.com/listenfwind/">zzzzMing -大数据技术</a>
</h1>
<h2>
蟹六跪而二螯,非蛇鳝之,无可寄托者,用心躁也
</h2>




		
	</div><!--end: blogTitle 博客的标题和副标题 -->
	<div id="navigator">
		
<ul id="navList">
<li><a id="blog_nav_sitehome" class="menu" href="https://www.cnblogs.com/">
博客园</a>
</li>
<li>
<a id="blog_nav_myhome" class="menu" href="https://www.cnblogs.com/listenfwind/">
首页</a>
</li>
<li>

<a id="blog_nav_newpost" class="menu" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">
新随笔</a>
</li>
<li>
<a id="blog_nav_contact" class="menu" href="https://msg.cnblogs.com/send/zzzzMing">
联系</a></li>
<li>
<a id="blog_nav_rss" class="menu" href="javascript:void(0)" data-rss="https://www.cnblogs.com/listenfwind/rss/">
订阅</a>
<!--<partial name="./Shared/_XmlLink.cshtml" model="Model" /></li>--></li>
<li>
<a id="blog_nav_admin" class="menu" href="https://i.cnblogs.com/">
管理</a>
</li>
</ul>


		<div class="blogStats">
			
			<span id="stats_post_count">随笔 - 
75&nbsp; </span>
<span id="stats_article_count">文章 - 
0&nbsp; </span>
<span id="stats-comment_count">评论 - 
77</span>

			
		</div><!--end: blogStats -->
	</div><!--end: navigator 博客导航栏 -->
</div><!--end: header 头部 -->

<div id="main">
	<div id="mainContent">
	<div class="forFlow">
		<div id="post_detail">
    <!--done-->
    <div id="topics">
        <div class="post">
            <h1 class = "postTitle">
                
<a id="cb_post_title_url" class="postTitle2" href="https://www.cnblogs.com/listenfwind/p/12767896.html">Spark SQL源码解析（五）SparkPlan准备和执行阶段</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
    <p>Spark SQL原理解析前言：</p>
<p><a href="https://www.cnblogs.com/listenfwind/p/12724381.html">Spark SQL源码剖析（一）SQL解析框架Catalyst流程概述</a></p>
<p><a href="https://www.cnblogs.com/listenfwind/p/12735833.html">Spark SQL源码解析（二）Antlr4解析Sql并生成树</a></p>
<p><a href="https://www.cnblogs.com/listenfwind/p/12795934.html">Spark SQL源码解析（三）Analysis阶段分析</a></p>
<p><a href="https://www.cnblogs.com/listenfwind/p/12886205.html">Spark SQL源码解析（四）Optimization和Physical Planning阶段解析</a></p>
<h1 id="sparkplan准备阶段介绍">SparkPlan准备阶段介绍</h1>
<p>前面经过千辛万苦，终于生成可实际执行的SparkPlan（即PhysicalPlan）。但在真正执行前，还需要做一些准备工作，包括在必要的地方插入一些shuffle作业，在需要的地方进行数据格式转换等等。</p>
<p>这部分内容都在org.apache.spark.sql.execution.QueryExecution类中。我们看看代码</p>
<pre><code>class QueryExecution(val sparkSession: SparkSession, val logical: LogicalPlan) {
  ......其他代码
  lazy val executedPlan: SparkPlan = prepareForExecution(sparkPlan)
  
  //调用下面的preparations，然后使用foldLeft遍历preparations中的Rule并应用到SparkPlan
  protected def prepareForExecution(plan: SparkPlan): SparkPlan = {
    preparations.foldLeft(plan) { case (sp, rule) =&gt; rule.apply(sp) }
  }

  /** A sequence of rules that will be applied in order to the physical plan before execution. */
  //定义各个Rule
  protected def preparations: Seq[Rule[SparkPlan]] = Seq(
    PlanSubqueries(sparkSession),
    EnsureRequirements(sparkSession.sessionState.conf),
    CollapseCodegenStages(sparkSession.sessionState.conf),
    ReuseExchange(sparkSession.sessionState.conf),
    ReuseSubquery(sparkSession.sessionState.conf))
  ......其他代码
}
</code></pre>
<p>准备阶段是去调用prepareForExecution方法，而prepareForExecution也简单，还是我们早先看过的Rule那一套东西。定义一系列的Rule，然后让Rule去匹配SparkPlan然后转换一遍。</p>
<p>这里在于每条Rule都是干嘛用的，这里介绍一下吧。</p>
<h4 id="plansubqueriessparksession">PlanSubqueries(sparkSession)</h4>
<p>生成子查询，在比较早的版本，Spark SQL还是不支持子查询的，不过现在加上了，这条Rule其实是对子查询的SQL新生成一个QueryExecution（就是我们一直分析的这个流程），还记得QueryExecution里面的变量基本都是懒加载的吧，这些不会立即执行，都是到最后一并执行的，说白了就有点递归的意思。</p>
<h4 id="ensurerequirementssparksessionsessionstateconf">EnsureRequirements(sparkSession.sessionState.conf)</h4>
<p>这条是比较重要的，代码量也多。主要就是验证输出的分区（partition）和我们要的分区是不是一样，不一样那自然需要加入shuffle处理重分区，如果有排序需求还会排序。</p>
<h3 id="collapsecodegenstages">CollapseCodegenStages</h3>
<p>这个是和一个优化相关的，先介绍下相关背景。Whole stage Codegen在一些MPP数据库被用来提高性能，主要就是将一串的算子，转换成一段代码（Spark sql转换成java代码），从而提高性能。比如下图，一串的算子操作，可以转换成一个java方法，这一一来性能会有一定的提升。<br>
<img src="https://img2020.cnblogs.com/blog/1011838/202004/1011838-20200424155608080-1337244720.png" alt=""></p>
<p>这一步就是在支持Codegen的SparkPlan上添加一个WholeStageCodegenExec，不支持Codegen的SparkPlan则会添加一个InputAdapter。这一点在下面看preparations阶段结果的时候能看到，还有这个优化是默认开启的。</p>
<h4 id="reuseexchange和reusesubquery">ReuseExchange和ReuseSubquery</h4>
<p>这两个都是大概同样的功能就放一块说了。首先Exchange是对shuffle如何进行的描述，可以理解为就是shuffle吧。</p>
<p>这里的ReuseExchange是一个优化措施，去找有重复的Exchange的地方，然后将结果替换过去，避免重复计算。</p>
<p>ReuseSubquery也是同样的道理，如果一条SQL语句中有多个相同的子查询，那么是不会重复计算的，会将计算的结果直接替换到重复的子查询中去，提高性能。</p>
<p>这里我略过了CollapseCodegenStages，这部分比较复杂，也没什么时间看，就先跳过了，大概知道这个东西是一个优化措施就行了。</p>
<p>那再来看看这一阶段后，示例代码会变成什么样吧，先看示例代码：</p>
<pre><code>    //生成DataFrame
    val df = Seq((1, 1)).toDF(&quot;key&quot;, &quot;value&quot;)
    df.createOrReplaceTempView(&quot;src&quot;)
    //调用spark.sql
    val queryCaseWhen = sql(&quot;select key from src &quot;)
</code></pre>
<p>结果生成如下：</p>
<pre><code>Project [_1#2 AS key#5]
+- LocalTableScan [_1#2, _2#3]
</code></pre>
<p>好吧这里看还是和之前Optimation阶段一样，不过断点看就不大一样了。<br>
<img src="https://img2020.cnblogs.com/blog/1011838/202004/1011838-20200424153815248-1329999930.png" alt=""></p>
<p>由于我们的SQL比较简单，所以只多了两个SparkPlan，就是WholeStageCodegenExec和InputAdapter，和上面说的是一致的！</p>
<p>OK，经过以上的准备之后，就要开始最后的执行阶段了。</p>
<h1 id="sparkplan执行生成rdd阶段">SparkPlan执行生成RDD阶段</h1>
<p>依旧是在QueryExecution里面，</p>
<pre><code>class QueryExecution(val sparkSession: SparkSession, val logical: LogicalPlan) {
  ......其他代码
  lazy val toRdd: RDD[InternalRow] = executedPlan.execute()
  ......其他代码
}
</code></pre>
<p>这里实际上是调用了之前生成的SparkPlan的execute()方法，这个方法最终会再调用它的doExecute()方法，而这个方法是各个子类自己实现的，也就是说，不同的SparkPlan执行的doExecute()是不一样的。</p>
<p>通过上面的阶段，我们得到了一棵4层的树，不过其中WholeStageCodegenExec和InputAdapter是为Codegen优化生成的，这里就不讨论了，忽略这两个其实结果是一样的。也就是说这里只介绍ProjectExec和LocalTableScanExec两个SparkPlan的doExecute()方法。</p>
<p>先是ProjectExec这个SparkPlan，我们看看它的doExecute()代码。</p>
<pre><code>case class ProjectExec(projectList: Seq[NamedExpression], child: SparkPlan)
  extends UnaryExecNode with CodegenSupport {
   ......其他代码
  protected override def doExecute(): RDD[InternalRow] = {
    child.execute().mapPartitionsWithIndexInternal { (index, iter) =&gt;
      val project = UnsafeProjection.create(projectList, child.output,
        subexpressionEliminationEnabled)
      project.initialize(index)
      iter.map(project)
    }
  }
  ......其他代码
}
</code></pre>
<p>可以看到它是先递归去调用child（也就是LocalTableScanExec）的doExecute()方法，还是得先去看看LocalTableScanExec生成什么东西呀。</p>
<pre><code>case class LocalTableScanExec(
    output: Seq[Attribute],
    @transient rows: Seq[InternalRow]) extends LeafExecNode {
  ......其他代码
	
  private lazy val rdd = sqlContext.sparkContext.parallelize(unsafeRows, numParallelism)
  
  protected override def doExecute(): RDD[InternalRow] = {
    val numOutputRows = longMetric(&quot;numOutputRows&quot;)
    rdd.map { r =&gt;
      numOutputRows += 1
      r
    }
  }
	
  ......其他代码
	
</code></pre>
<p>可以看到最底层的rdd就是在这里实现的，LocalTableScanExec一开始就会生成一个lazy的rdd，在需要的时候返回。而在doExecute()方法中的numOutputRows可以理解为仅是一个测量值，暂时不用理会。总之这里我们就发现LocalTableScanExec的doExecute()其实就是返回一个parallelize生成的rdd。然后再回到ProjectExec去。</p>
<p>它调用child.execute().mapPartitionsWithIndexInternal {......}，这里的mapPartitionsWithIndexInternal和rdd的mapPartitionsWithIndex是类似的，区别只在于mapPartitionsWithIndexInternal只会在内部模块使用，如果有童鞋不明白mapPartitionsWithIndex这个API，可以百度查查看。然后重点看mapPartitionsWithIndexInternal的内部逻辑。</p>
<pre><code>child.execute().mapPartitionsWithIndexInternal { (index, iter) =&gt;
  val project = UnsafeProjection.create(projectList, child.output,
    subexpressionEliminationEnabled)
  project.initialize(index)
  iter.map(project)
}
</code></pre>
<p>这里最后一行iter.map(project)，其实还是scala的语法糖，实际大概是这样iter.map(i =&gt; project.apply(i))。就是调用project的apply方法，对每行数据处理。然后通过追踪，可以发现project的实例是InterpretedUnsafeProjection，我们看看它的apply方法。</p>
<pre><code>class InterpretedUnsafeProjection(expressions: Array[Expression]) extends UnsafeProjection {
  ......其他代码
  override def apply(row: InternalRow): UnsafeRow = {
    // Put the expression results in the intermediate row.
    var i = 0
    while (i &lt; numFields) {
      values(i) = expressions(i).eval(row)
      i += 1
    }

    // Write the intermediate row to an unsafe row.
    rowWriter.reset()
    writer(intermediate)
    rowWriter.getRow()
  }
  
  ......其他代码
</code></pre>
<p>这里其实重点在最后三行，就是将结果写入到result row，再返回回去。当执行完毕的时候，就会得到最终的RDD[InternalRow]，再剩下的，就交给spark core去处理了。</p>
<h3 id="小结">小结</h3>
<p>OK，那到这里基本就把Spark整个流程给讲完了，回顾一下整个流程。<br>
<img src="https://img2020.cnblogs.com/blog/1011838/202004/1011838-20200418114451800-50235194.png" alt="catalyst流程"></p>
<p>其实可以发现流程是挺简单的，很多其他SQL解析框架（比如calcite）也是类似的流程，只是在设计上在某些方面的取舍会有偏差。而后深入到代码的时候容易陷入一些细节中，当然这几篇也省略了很多细节，很多时候细节才是真正精髓的地方，以后有如果涉及到的时候再写文章讨论吧（/偷笑）。如果在开放过程中涉及到SQL解析这方面的开放，应该都会是在优化方面，也就是Optimization阶段增加或处理Rule，这块就需要对代数优化理论和代码有一些了解了。</p>
<p>限于本人水平，介绍spark sql的这几篇文章难免有疏漏和不足的地方，欢迎在评论区评论，先谢过了~~</p>
<p>以上~</p>

</div>
<div id="MySignature"></div>
<div class="clear"></div>
<div id="blog_post_info_block">
    <div id="blog_post_info"></div>
    <div class="clear"></div>
    <div id="post_next_prev"></div>
</div>
            </div>
            <div class="postDesc">posted @ 
<span id="post-date">2020-05-27 18:43</span>&nbsp;
<a href="https://www.cnblogs.com/listenfwind/">zzzzMing</a>&nbsp;
阅读(<span id="post_view_count">...</span>)&nbsp;
评论(<span id="post_comment_count">...</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=12767896" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(12767896);return false;">收藏</a></div>
        </div>
	    
	    
    </div><!--end: topics 文章、评论容器-->
</div>
<script src="https://common.cnblogs.com/highlight/9.12.0/highlight.min.js"></script>
<script>markdown_highlight();</script>
<script>
    var allowComments = true, cb_blogId = 303786, cb_blogApp = 'listenfwind', cb_blogUserGuid = '6569457a-f565-e611-9fc1-ac853d9f53cc';
    var cb_entryId = 12767896, cb_entryCreatedDate = '2020-05-27 18:43', cb_postType = 1; 
    loadViewCount(cb_entryId);
    loadSideColumnAd();
</script><a name="!comments"></a>
<div id="blog-comments-placeholder"></div>
<script>
    var commentManager = new blogCommentManager();
    commentManager.renderComments(0);
</script>

<div id="comment_form" class="commentform">
    <a name="commentform"></a>
    <div id="divCommentShow"></div>
    <div id="comment_nav"><span id="span_refresh_tips"></span><a href="javascript:void(0);" onclick="return RefreshCommentList();" id="lnk_RefreshComments" runat="server" clientidmode="Static">刷新评论</a><a href="#" onclick="return RefreshPage();">刷新页面</a><a href="#top">返回顶部</a></div>
    <div id="comment_form_container"></div>
    <div class="ad_text_commentbox" id="ad_text_under_commentbox"></div>
    <div id="ad_t2"></div>
    <div id="opt_under_post"></div>
    <script async="async" src="https://www.googletagservices.com/tag/js/gpt.js"></script>
    <script>
        var googletag = googletag || {};
        googletag.cmd = googletag.cmd || [];
    </script>
    <script>
        googletag.cmd.push(function () {
            googletag.defineSlot("/1090369/C1", [300, 250], "div-gpt-ad-1546353474406-0").addService(googletag.pubads());
            googletag.defineSlot("/1090369/C2", [468, 60], "div-gpt-ad-1539008685004-0").addService(googletag.pubads());
            googletag.pubads().enableSingleRequest();
            googletag.enableServices();
        });
    </script>
    <div id="cnblogs_c1" class="c_ad_block">
        <div id="div-gpt-ad-1546353474406-0" style="height:250px; width:300px;"></div>
    </div>
    <div id="under_post_news"></div>
    <div id="cnblogs_c2" class="c_ad_block">
        <div id="div-gpt-ad-1539008685004-0" style="height:60px; width:468px;"></div>
    </div>
    <div id="under_post_kb"></div>
    <div id="HistoryToday" class="c_ad_block"></div>
    <script type="text/javascript">
        fixPostBody();
        deliverBigBanner();
setTimeout(function() { incrementViewCount(cb_entryId); }, 50);        deliverAdT2();
        deliverAdC1();
        deliverAdC2();
        loadNewsAndKb();
        loadBlogSignature();
LoadPostCategoriesTags(cb_blogId, cb_entryId);        LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
        GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);
        loadOptUnderPost();
        GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);
    </script>
</div>
	</div><!--end: forFlow -->
	</div><!--end: mainContent 主体内容容器-->

	<div id="sideBar">
		<div id="sideBarMain">
			
<div id="sidebar_news" class="newsItem">
            <script>loadBlogNews();</script>
</div>

<div id="sidebar_ad"></div>
			<div id="blog-calendar" style="display:none"></div><script>loadBlogDefaultCalendar();</script>
			
			<div id="leftcontentcontainer">
				<div id="blog-sidecolumn"></div>
                    <script>loadBlogSideColumn();</script>
			</div>
			
		</div><!--end: sideBarMain -->
	</div><!--end: sideBar 侧边栏容器 -->
	<div class="clear"></div>
	</div><!--end: main -->
	<div class="clear"></div>
	<div id="footer">
		<!--done-->
Copyright &copy; 2020 zzzzMing
<br /><span id="poweredby">Powered by .NET Core on Kubernetes</span>



	</div><!--end: footer -->
</div><!--end: home 自定义的最大容器 -->


    
</body>
</html>